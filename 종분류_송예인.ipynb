{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "종분류-송예인.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGsL1SX0Ke/zrV7CaXzDG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Semy-sudo/CATMOS/blob/yein/%EC%A2%85%EB%B6%84%EB%A5%98_%EC%86%A1%EC%98%88%EC%9D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pGbUIT1nvhE",
        "colab_type": "code",
        "outputId": "b0154849-d78c-4847-88b5-dffa73d3734b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#구글 드라이브로 부터 파일 가져오기\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUYSTWHMa1ey",
        "colab_type": "code",
        "outputId": "3f692d8d-4e8e-4fef-be8e-8f1d2c03e4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exX5febFa1qh",
        "colab_type": "code",
        "outputId": "a02e3370-1911-4749-9150-999f8a84f88d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.5.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/external/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.5.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/alloc.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/argcheck.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/bootstrap.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/channel.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/checks.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/collectives.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/comm.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/core.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/cpuset.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/debug.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/devcomm.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/enqueue.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/graph.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/group.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/ibvwrap.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/info.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/nccl_net.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/net.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/nvmlwrap.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/param.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/shm.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/socket.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/transport.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/trees.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/include_hdrs/utils.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/nccl/third_party/nccl/nccl.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/src_hdrs/include/collectives.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/_virtual_includes/src_hdrs/nccl.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/src/graph/rings.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/src/graph/topo.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/src/include/collectives.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/external/nccl_archive/src/nccl.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/convert/convert_graph.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/convert/logger_registry.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/convert/utils.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/plugin/trt_plugin.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/segment/segment.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/segment/union_find.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/utils/trt_allocator.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/utils/trt_engine_instance.pb.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/utils/trt_int8_calibrator.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/utils/trt_logger.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/utils/trt_lru_cache.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/compiler/tf2tensorrt/utils/trt_shape_optimization_profiles.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/kernels/collective_nccl.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/kernels/collective_nccl_broadcaster.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/kernels/collective_nccl_gatherer.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/kernels/collective_nccl_reducer.h\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/nccl/nccl_manager.h\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-1.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZt0EVK4bAXi",
        "colab_type": "code",
        "outputId": "9bdd8f4d-ace6-41e3-cc33-a7df6f0bc734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        }
      },
      "source": [
        "!pip install tensorflow==1.2"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/55/7995cc1e9e60fa37ea90e6777d832e75026fde5c6109215d892aaff2e9b7/tensorflow-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (35.0MB)\n",
            "\u001b[K     |████████████████████████████████| 35.0MB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (3.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.0.1)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.16.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (0.34.2)\n",
            "Collecting markdown==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/99/288a81a38526a42c98b5b9832c6e339ca8d5dd38b19a53abfac7c8037c7f/Markdown-2.2.0.tar.gz (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 39.3MB/s \n",
            "\u001b[?25hCollecting backports.weakref==1.0rc1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f7/ae34b6818b603e264f26fe7db2bd07850ce331ce2fde74b266d61f4a2d87/backports.weakref-1.0rc1-py3-none-any.whl\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (0.9999999)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.2) (46.3.0)\n",
            "Building wheels for collected packages: markdown\n",
            "  Building wheel for markdown (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markdown: filename=Markdown-2.2.0-cp36-none-any.whl size=136308 sha256=10ae1043ad21368d00cf5c710d479d9fefdc19c352ab1241166dc0f713e5b560\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/52/17/f0af18e3e0ec6fa60b361ffed15b4c3468f6f3bcdb87fbe079\n",
            "Successfully built markdown\n",
            "\u001b[31mERROR: tensorflow-tensorboard 1.5.1 has requirement markdown>=2.6.8, but you'll have markdown 2.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.2.1 has requirement markdown>=2.6.8, but you'll have markdown 2.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: markdown, backports.weakref, tensorflow\n",
            "  Found existing installation: Markdown 3.2.2\n",
            "    Uninstalling Markdown-3.2.2:\n",
            "      Successfully uninstalled Markdown-3.2.2\n",
            "Successfully installed backports.weakref-1.0rc1 markdown-2.2.0 tensorflow-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh7ckcT3bQQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get -qq install -y libfluidsynth1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCbJr_oYlHU_",
        "colab_type": "code",
        "outputId": "b900dc8c-f6b7-4175-b4a4-077c2768a12c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "from PIL import Image # PIL 이미지 처리 기능 패키지\n",
        "import os, glob, numpy as np \n",
        "from sklearn.model_selection import train_test_split #데이터 분할해주는 함수 train, validation, test\n",
        "\n",
        "#경로 가져오기\n",
        "caltech_dir = \"/content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train\"\n",
        "#작은 카테고리\n",
        "categories = [\"Siberia\", \"sham\", \"Rusianblue\",\"ragdoll\",\"Persian\",\"boombey\",\"Bengal\"]\n",
        "nb_classes = len(categories)  # len함수: 문자열 길이 반환\n",
        "\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "X = [] \n",
        "y = []\n",
        "\n",
        "for idx, cat in enumerate(categories):\n",
        "    \n",
        "    #one-hot 돌리기.\n",
        "    label = [0 for i in range(nb_classes)] \n",
        "    label[idx] = 1\n",
        "    \n",
        "    #cat = \"Rusianblue\", \"봄베이\", \"Persian\", \"Siberia\"  8 개 중 하나\n",
        "    image_dir = caltech_dir + \"/\" + cat\n",
        "    files = glob.glob(image_dir+\"/*.jpg\") #glob함수: 파일명을 문자열로 반환\n",
        "    print(cat, \" 파일 길이 : \", len(files))\n",
        "    for i, f in enumerate(files):\n",
        "        img = Image.open(f)\n",
        "        img = img.convert(\"RGB\")\n",
        "        img = img.resize((image_w, image_h))\n",
        "        #변환한 이미지 파일을 data로 \n",
        "        data = np.asarray(img) #numpy 배열로 변환\n",
        "\n",
        "        X.append(data)\n",
        "        y.append(label)\n",
        "\n",
        "        if i % 700 == 0:\n",
        "            print(cat, \" : \", f) #종류 : 데이터 개수\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "#1 0 0 0 이면 러시안블루\n",
        "#0 1 0 0 이면 봄베이 이런식\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "xy = (X_train, X_test, y_train, y_test)\n",
        "#경로에 train test data 를 multi_image_data 로 저장\n",
        "np.save(\"/content/drive/My Drive/Colab Notebooks/numpy_data/multi_image_data.npy\", xy) #xy 배열을 해당 경로에 저장\n",
        "\n",
        "print(\"ok\", len(y))\n",
        "print(\"ok\", len(X))\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Siberia  파일 길이 :  184\n",
            "Siberia  :  /content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/Siberia/Siberia (76).jpg\n",
            "sham  파일 길이 :  199\n",
            "sham  :  /content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/sham/siam (1).jpg\n",
            "Rusianblue  파일 길이 :  207\n",
            "Rusianblue  :  /content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/Rusianblue/러시안블루2.jpg\n",
            "ragdoll  파일 길이 :  200\n",
            "ragdoll  :  /content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/ragdoll/ragdoll1.jpg\n",
            "Persian  파일 길이 :  190\n",
            "Persian  :  /content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/Persian/persian1.jpg\n",
            "boombey  파일 길이 :  191\n",
            "boombey  :  /content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/boombey/bombay001.jpg\n",
            "Bengal  파일 길이 :  99\n",
            "Bengal  :  /content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/Bengal/bengal (37).jpg\n",
            "ok 1270\n",
            "ok 1270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzGthTECgDbt",
        "colab_type": "code",
        "outputId": "db469020-37bc-428d-80c0-875007a78c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "pip install keras==2.2.1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.2.1 in /usr/local/lib/python3.6/dist-packages (2.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (1.4.1)\n",
            "Collecting keras-preprocessing==1.0.2\n",
            "  Using cached https://files.pythonhosted.org/packages/71/26/1e778ebd737032749824d5cba7dbd3b0cf9234b87ab5ec79f5f0403ca7e9/Keras_Preprocessing-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: keras-applications==1.0.4 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (1.0.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (1.16.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (1.12.0)\n",
            "\u001b[31mERROR: tensorflow-gpu 2.2.0 has requirement keras-preprocessing>=1.1.0, but you'll have keras-preprocessing 1.0.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-preprocessing\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "Successfully installed keras-preprocessing-1.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras_preprocessing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk56Zi3Fhbqh",
        "colab_type": "code",
        "outputId": "add4af16-995c-4cac-e153-69e6376d6836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install numpy==1.16.2\n",
        "import numpy as np"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.2 in /usr/local/lib/python3.6/dist-packages (1.16.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBm5dBtkZXA_",
        "colab_type": "code",
        "outputId": "d861d8e8-9dfa-4551-dcb6-be3020c68d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install --upgrade tensorflow-gpu"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Collecting keras-preprocessing>=1.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.29.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.0.post3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (46.3.0)\n",
            "Collecting markdown>=2.6.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 1.2.0 has requirement markdown==2.2.0, but you'll have markdown 3.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: keras 2.2.1 has requirement keras-preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-preprocessing, markdown\n",
            "  Found existing installation: Keras-Preprocessing 1.0.2\n",
            "    Uninstalling Keras-Preprocessing-1.0.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.0.2\n",
            "  Found existing installation: Markdown 2.2.0\n",
            "    Uninstalling Markdown-2.2.0:\n",
            "      Successfully uninstalled Markdown-2.2.0\n",
            "Successfully installed keras-preprocessing-1.1.2 markdown-3.2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras_preprocessing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlOb9aCdll6g",
        "colab_type": "code",
        "outputId": "fb91d14f-afee-4524-c43b-fb0659b2696d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "\n",
        "import os, glob, numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend.tensorflow_backend as K   \n",
        "\n",
        "import tensorflow as tf   #GPU 구현에 있어서 메모리를 절약해서 사용하기 위한 코드\n",
        "#config = tf.ConfigProto()\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "#session = tf.Session(config=config)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = np.load('/content/drive/My Drive/Colab Notebooks/numpy_data/multi_image_data.npy',allow_pickle=True)\n",
        "print(X_train.shape)\n",
        "print(y_train[:10])\n",
        "print(X_train.shape)\n",
        "print(X_train.shape[1:])\n",
        "print(X_test.shape)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(952, 64, 64, 3)\n",
            "[[0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [1 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]]\n",
            "(952, 64, 64, 3)\n",
            "(64, 64, 3)\n",
            "(318, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV9JalmhfRM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"러시안블루\", \"봄베이\", \"Persian\", \"Siberia\"\n",
        "categories = [\"Siberia\", \"sham\", \"Rusianblue\",\"ragdoll\",\"Persian\",\"boombey\",\"Bengal\"]\n",
        "nb_classes = len(categories) \n",
        "\n",
        "#일반화\n",
        "X_train = X_train.astype(float) / 255\n",
        "X_test = X_test.astype(float) / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaDHaKTo30eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#모델 학습하기\n",
        "with K.tf_ops.device('/device:GPU:0'):\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                    activation='relu',\n",
        "                    input_shape=(64,64,3)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "    \n",
        "\n",
        "    #model = Sequential()\n",
        "    #model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
        "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    #model.add(Dropout(0.25))\n",
        "    \n",
        "    #model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
        "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    #model.add(Dropout(0.25))\n",
        "    \n",
        "    #model.add(Flatten())\n",
        "    #model.add(Dense(256, activation='relu'))\n",
        "    #model.add(Dropout(0.5))\n",
        "    #model.add(Dense(nb_classes, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model_dir = './model'\n",
        "    \n",
        "    if not os.path.exists(model_dir):\n",
        "        os.mkdir(model_dir)\n",
        "    \n",
        "    model_path = model_dir + '/multi_img_classification.model'\n",
        "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVz-EV1g32uJ",
        "colab_type": "code",
        "outputId": "94eb0135-f3e5-45a3-8c8a-a3264f0ed59a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "\n",
        "model.summary()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 60, 60, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 58, 58, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 56, 56, 64)        73792     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               6422656   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 6,590,599\n",
            "Trainable params: 6,590,599\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gJrLBVA35wr",
        "colab_type": "code",
        "outputId": "c72bfb8a-dca6-4bce-ebd8-1223578ad46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        }
      },
      "source": [
        "#epoch 너무 높아도 정확도 안나옴\n",
        "\n",
        "\n",
        "#데이터셋이 적어서 validation을 그냥 test 데이터로 했습니다. \n",
        "#데이터셋이 충분하시면 이렇게 하시지 마시고 validation_split=0.2 이렇게 하셔서 테스트 셋으로 나누시길 권장합니다.\n",
        "\n",
        "#모델 학습시키기\n",
        "#입력데이터/라벨값/ 가중치 /학습횟수 /비슷한게 5번이상나오면 학습중단\n",
        "\n",
        "#model.fit_generator(\n",
        "#       (X_train, y_train),\n",
        "#        steps_per_epoch=15,\n",
        "#        epochs=50,\n",
        "#        validation_data=(X_test, y_test),\n",
        "#        callbacks=[checkpoint, early_stopping])\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 952 samples, validate on 318 samples\n",
            "Epoch 1/50\n",
            "952/952 [==============================] - 17s 17ms/step - loss: 1.8842 - accuracy: 0.3298 - val_loss: 1.4018 - val_accuracy: 0.4088\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.40180, saving model to ./model/multi_img_classification.model\n",
            "Epoch 2/50\n",
            "952/952 [==============================] - 16s 17ms/step - loss: 1.1751 - accuracy: 0.5525 - val_loss: 1.1312 - val_accuracy: 0.5818\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.40180 to 1.13120, saving model to ./model/multi_img_classification.model\n",
            "Epoch 3/50\n",
            "952/952 [==============================] - 16s 17ms/step - loss: 0.7847 - accuracy: 0.7395 - val_loss: 1.1884 - val_accuracy: 0.5912\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.13120\n",
            "Epoch 4/50\n",
            "952/952 [==============================] - 16s 17ms/step - loss: 0.5168 - accuracy: 0.8204 - val_loss: 1.3604 - val_accuracy: 0.5786\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.13120\n",
            "Epoch 5/50\n",
            "952/952 [==============================] - 16s 17ms/step - loss: 0.2947 - accuracy: 0.9086 - val_loss: 1.3876 - val_accuracy: 0.5975\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.13120\n",
            "Epoch 6/50\n",
            "952/952 [==============================] - 16s 17ms/step - loss: 0.1782 - accuracy: 0.9527 - val_loss: 1.5751 - val_accuracy: 0.6069\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.13120\n",
            "Epoch 7/50\n",
            "952/952 [==============================] - 16s 17ms/step - loss: 0.1419 - accuracy: 0.9664 - val_loss: 1.7873 - val_accuracy: 0.5723\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.13120\n",
            "Epoch 8/50\n",
            "952/952 [==============================] - 17s 17ms/step - loss: 0.0851 - accuracy: 0.9779 - val_loss: 2.1744 - val_accuracy: 0.5818\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.13120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLD6i6eD37pk",
        "colab_type": "code",
        "outputId": "d820f6b6-3461-4ed2-c466-a5ff9a067752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#epoch 너무 높아도 정확도 안나옴\n",
        "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "318/318 [==============================] - 1s 4ms/step\n",
            "정확도 : 0.5818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGPIj7py4FLV",
        "colab_type": "code",
        "outputId": "1a404081-a0f5-4e30-92bc-f8141d68ba7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "\n",
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "x_len = np.arange(len(y_loss))\n",
        "\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "#학습에 대한 오차< 검증용 오차"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "___-ne9j_ccv",
        "colab_type": "code",
        "outputId": "6690e0b3-edb8-4948-d34b-10f15d5c1197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob, numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "caltech_dir = \"/content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others_test\"\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "\n",
        "X = []\n",
        "filenames = []\n",
        "name = []\n",
        "files = glob.glob(caltech_dir+\"/*.*\")\n",
        "model = load_model('./model/multi_img_classification.model')\n",
        "cnt=0\n",
        "for i, f in enumerate(files):\n",
        "    #print(f)\n",
        "    img = Image.open(f)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((image_w, image_h))\n",
        "    data = np.asarray(img)\n",
        "    filenames.append(f)\n",
        "    X.append(data)\n",
        "    prediction = model.predict(data[np.newaxis])\n",
        "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "    \n",
        "    for i in prediction:\n",
        "      pre_ans = i.argmax()  # 예측 레이블\n",
        "      pre_ans_str = ''\n",
        "      # 순서 \"Rusianblue\", \"봄베이\", \"Persian\", \"Siberia\"\n",
        "      #\"Siberia\", \"sham\", \"Rusianblue\",\"ragdoll\",\"Persian\",\"boombey\",\"Bengal\"\n",
        "      if pre_ans == 0: pre_ans_str = \"Siberia\"\n",
        "      elif pre_ans == 1: pre_ans_str = \"sham\"\n",
        "      elif pre_ans == 2: pre_ans_str = \"Rusianblue\"\n",
        "      elif pre_ans == 1: pre_ans_str = \"ragdoll\"\n",
        "      elif pre_ans == 2: pre_ans_str = \"Persian\"\n",
        "      elif pre_ans == 1: pre_ans_str = \"boombey\"\n",
        "      else: pre_ans_str = \"Bengal\"\n",
        "      if i[0] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "      if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "      if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "      if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "      if i[4] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "      if i[5] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "      if i[6] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "\n",
        "\n",
        "      cnt += 1"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "해당 뱅갈1.jpg이미지는 sham로 추정됩니다.\n",
            "해당 뱅갈2.jpg이미지는 Bengal로 추정됩니다.\n",
            "해당 Siberia2.jpg이미지는 Siberia로 추정됩니다.\n",
            "해당 ragdoll18.jpg이미지는 Bengal로 추정됩니다.\n",
            "해당 ragdoll55.jpg이미지는 Bengal로 추정됩니다.\n",
            "해당 ragdoll30.jpg이미지는 Bengal로 추정됩니다.\n",
            "해당 ragdoll28.jpg이미지는 Bengal로 추정됩니다.\n",
            "해당 ragdoll82.jpg이미지는 Bengal로 추정됩니다.\n",
            "해당 ragdoll57.jpg이미지는 Bengal로 추정됩니다.\n",
            "해당 bengal (1).jpg이미지는 Bengal로 추정됩니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_iZMN4d4Hw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "96b1f3e6-d6a2-4142-c25f-f34264ce7043"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob, numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "caltech_dir = \"/content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others_test\"\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "X = []\n",
        "filenames = []\n",
        "files = glob.glob(caltech_dir+\"/*.*\")\n",
        "\n",
        "for i, f in enumerate(files):\n",
        "    img = Image.open(f)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((image_w, image_h))\n",
        "    data = np.asarray(img)\n",
        "    filenames.append(f)\n",
        "    X.append(data)\n",
        "    \n",
        "    \n",
        "model = load_model('./model/multi_img_classification.model')\n",
        "\n",
        "prediction = model.predict(X)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "cnt = 0\n",
        "\n",
        "#이 비교는 그냥 파일들이 있으면 해당 파일과 비교. 카테고리와 함께 비교해서 진행하는 것은 _4 파일.\n",
        "for i in prediction:\n",
        "    pre_ans = i.argmax()  # 예측 레이블\n",
        "    print(i)\n",
        "    print(pre_ans)\n",
        "    pre_ans_str = ''\n",
        "    # 순서 \"Rusianblue\", \"봄베이\", \"Persian\", \"Siberia\"\n",
        "    if pre_ans == 0: pre_ans_str = \"Rusianblue\"\n",
        "    elif pre_ans == 1: pre_ans_str = \"봄베이\"\n",
        "    elif pre_ans == 2: pre_ans_str = \"Persian\"\n",
        "    else: pre_ans_str = \"Siberia\"\n",
        "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
        "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    cnt += 1\n",
        "    # print(i.argmax()) #얘가 레이블 [1. 0. 0.] 이런식으로 되어 있는 것을 숫자로 바꿔주는 것.\n",
        "    # 즉 얘랑, 나중에 카테고리 데이터 불러와서 카테고리랑 비교를 해서 같으면 맞는거고, 아니면 틀린거로 취급하면 된다.\n",
        "    # 이걸 한 것은 _4.py에."
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-c353f7939b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/multi_img_classification.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"{0:0.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmaximum\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInteger\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMaximum\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspin\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m                 \u001b[0mwhen\u001b[0m \u001b[0musing\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0munspecified\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mto\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m                 \u001b[0mexecute\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmain\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model inputs are already set.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m' Numpy arrays instead. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 'The list you passed was: ' + str(data)[:200])\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             raise TypeError('Error when checking model ' + exception_prefix +\n\u001b[1;32m    111\u001b[0m                             \u001b[0;34m': data should be a Numpy array, or list/dict of '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 15 arrays: [array([[[237, 239, 236],\n        [236, 239, 236],\n        [236, 238, 235],\n        ...,\n        [240, 242, 241],\n        [236, 238, 237],\n        [237, 239, 237]],\n\n       [[243, 245, 244],\n        [..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKnZ4RZ88AB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(3)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "# 원본 이미지 위치 \n",
        "optInputPath = '/content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/Bengal'\n",
        "# 늘릴 이미지가 저장될 위치\n",
        "optOutputPath = '/content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/PlusBengal'\n",
        "\n",
        "# 이미지 크기 조정 비율\n",
        "optRescale = 1./255\n",
        "# 이미지 회전 \n",
        "optRotationRange=10\n",
        "# 이미지 수평 이동\n",
        "optWidthShiftRange=0.2\n",
        "# 이미지 수직 이동\n",
        "optHeightShiftRange=0.2\n",
        "# 이미지 밀림 강도 \n",
        "optShearRange=0.5\n",
        "# 이미지 확대/ 축소 \n",
        "optZoomRange=[0.9,2.2]\n",
        "# 이미지 수평 뒤집기 \n",
        "optHorizontalFlip = True \n",
        "# 이미지 수직 뒤집기 \n",
        "optVerticalFlip = True\n",
        "optFillMode='nearest'\n",
        "# 이미지당 늘리는 갯수 \n",
        "optNbrOfIncreasePerPic = 5\n",
        "# 배치 수 \n",
        "optNbrOfBatchPerPic = 5\n",
        "\n",
        "'''\n",
        " 총 개수 optNbrOfIncreasePerPic * optNbrOfBatchPerPic \n",
        " 예 >\n",
        " 사진 1장에 \n",
        " optNbrOfIncreasePerPic = 5\n",
        " optNbrOfBatchPerPic = 5\n",
        " = 1 * 5 * 5 = 25장 생성  \n",
        "'''\n",
        "\n",
        "# 데이터셋 불러오기\n",
        "train_datagen = ImageDataGenerator(rescale=optRescale, \n",
        "                                   rotation_range=optRotationRange,\n",
        "                                   width_shift_range=optWidthShiftRange,\n",
        "                                   height_shift_range=optHeightShiftRange,\n",
        "                                   shear_range=optShearRange,\n",
        "                                   zoom_range=optZoomRange,\n",
        "                                   horizontal_flip=optHorizontalFlip,\n",
        "                                   vertical_flip=optVerticalFlip,\n",
        "                                   fill_mode=optFillMode)\n",
        "'''\n",
        "폴더가 존재하는지 확인하고\n",
        "없다면 생성 \n",
        "'''\n",
        "def checkFoler(path):\n",
        "    try:\n",
        "        if not(os.path.isdir(path)):\n",
        "            os.makedirs(os.path.join(path))\n",
        "    except OSError as e:\n",
        "        if e.errno != errno.EEXIST:                        \n",
        "            raise            \n",
        "\n",
        "def increaseImage(path ,folder):\n",
        "    for index in range(0,optNbrOfIncreasePerPic):                                   \n",
        "        img = load_img(path)\n",
        "        x = img_to_array(img)\n",
        "        x = x.reshape((1,) + x.shape)\n",
        "        i = 0\n",
        "        # 부풀리는 이미지를 저장할 폴다가 존재하는지 확인\n",
        "        # 없다면 생성 \n",
        "        checkFoler(optOutputPath+folder)               \n",
        "        print('index : ' + str(index))\n",
        "        for batch in train_datagen.flow(x, batch_size=1, save_to_dir=optOutputPath+folder, save_prefix='bengal', save_format='jpg'):\n",
        "            i += 1\n",
        "            print(folder + \" \" + str(i))\n",
        "            if i >= optNbrOfBatchPerPic: \n",
        "                break\n",
        "\n",
        "def generator(dirName):\n",
        "    checkFoler(optOutputPath)\n",
        "    try:\n",
        "        fileNames = os.listdir(dirName)\n",
        "        for fileName in fileNames:\n",
        "            fullFileName = os.path.join(dirName, fileName)\n",
        "            if os.path.isdir(fullFileName):                \n",
        "                generator(fullFileName)\n",
        "            else:\n",
        "                # 확장자 \n",
        "                ext = os.path.splitext(fullFileName)[-1]\n",
        "                # 폴더 이름 \n",
        "                folderName = os.path.splitext(fullFileName)[0].split('/')[-2]\n",
        "                if(ext == '.jpg'):                    \n",
        "                    increaseImage(fullFileName, folderName)               \n",
        "                    \n",
        "    except PermissionError:\n",
        "        pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "        generator(optInputPath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVEmlSNkF9BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import cv2\n",
        "\n",
        "CAM_ID = 0\n",
        "def capture(camid = CAM_ID):\n",
        "    cam = cv2.VideoCapture(camid)\n",
        "    if cam.isOpened() == False:\n",
        "        print ('cant open the cam (%d)' % camid)\n",
        "        return None\n",
        "\n",
        "    ret, frame = cam.read()\n",
        "    if frame is None:\n",
        "        print ('frame is not exist')\n",
        "        return None\n",
        "    \n",
        "    # png로 압축 없이 영상 저장 \n",
        "    cv2.imwrite('messigray.png',frame, params=[cv2.IMWRITE_PNG_COMPRESSION,0])\n",
        "    cam.release()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    capture()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3JAY80arbrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "cap = cv2.VideoCapture(0)\n",
        "cap.set(3,640) # set Width\n",
        "cap.set(4,480) # set Height\n",
        "while(True):\n",
        "    ret, frame = cap.read()\n",
        "    frame = cv2.flip(frame, -1) # Flip camera vertically\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    cv2.imshow('frame', frame)\n",
        "    cv2.imshow('gray', gray)\n",
        "    \n",
        "    k = cv2.waitKey(30) & 0xff\n",
        "    if k == 27: # press 'ESC' to quit\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGdToVCysI0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}