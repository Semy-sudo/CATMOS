{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "종분류-송예인.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvZodF6O3k8pqrcdgiL87Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Semy-sudo/CATMOS/blob/yein/%EC%A2%85%EB%B6%84%EB%A5%98_%EC%86%A1%EC%98%88%EC%9D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvpnOv35Y6bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#구글 드라이브로 부터 파일 가져오기\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPSHzWlchWre",
        "colab_type": "code",
        "outputId": "e863bad9-7ca3-4632-f97c-9cac4de375b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#텐서 버전 확인\n",
        "!pip show tensorflow\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.5.0\n",
            "Summary: TensorFlow helps the tensors flow\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: opensource@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: numpy, wheel, protobuf, tensorflow-tensorboard, six, absl-py\n",
            "Required-by: fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "749WsV0YjFqY",
        "colab_type": "code",
        "outputId": "106ed0a4-bd15-4cf7-8d41-d86cd4a578a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        }
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/be/679ce5254a8c8d07470efb4a4c00345fae91f766e64f1c2aece8796d7218/tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 27kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Collecting keras-preprocessing>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.3.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "\u001b[31mERROR: keras 2.2.1 has requirement keras-preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-preprocessing, tensorflow\n",
            "  Found existing installation: Keras-Preprocessing 1.0.2\n",
            "    Uninstalling Keras-Preprocessing-1.0.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.0.2\n",
            "  Found existing installation: tensorflow 1.5.0\n",
            "    Uninstalling tensorflow-1.5.0:\n",
            "      Successfully uninstalled tensorflow-1.5.0\n",
            "Successfully installed keras-preprocessing-1.1.2 tensorflow-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQsqmqfZjKpp",
        "colab_type": "code",
        "outputId": "7ed81bc5-9377-4923-a8f7-c2569e6e0a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        }
      },
      "source": [
        "!pip install tensorflow==1.2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/55/7995cc1e9e60fa37ea90e6777d832e75026fde5c6109215d892aaff2e9b7/tensorflow-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (35.0MB)\n",
            "\u001b[K     |████████████████████████████████| 35.0MB 105kB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.0.1)\n",
            "Collecting backports.weakref==1.0rc1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f7/ae34b6818b603e264f26fe7db2bd07850ce331ce2fde74b266d61f4a2d87/backports.weakref-1.0rc1-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.16.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.12.0)\n",
            "Collecting markdown==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/99/288a81a38526a42c98b5b9832c6e339ca8d5dd38b19a53abfac7c8037c7f/Markdown-2.2.0.tar.gz (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.5.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (0.34.2)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (0.9999999)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.2) (46.3.0)\n",
            "Building wheels for collected packages: markdown\n",
            "  Building wheel for markdown (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markdown: filename=Markdown-2.2.0-cp36-none-any.whl size=136331 sha256=86e21f919f9c3737b058bdc8e763e51233678612447c13c1f728280aa00cc8f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/52/17/f0af18e3e0ec6fa60b361ffed15b4c3468f6f3bcdb87fbe079\n",
            "Successfully built markdown\n",
            "\u001b[31mERROR: tensorflow-tensorboard 1.5.1 has requirement markdown>=2.6.8, but you'll have markdown 2.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.2.1 has requirement markdown>=2.6.8, but you'll have markdown 2.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: backports.weakref, markdown, tensorflow\n",
            "  Found existing installation: Markdown 3.2.1\n",
            "    Uninstalling Markdown-3.2.1:\n",
            "      Successfully uninstalled Markdown-3.2.1\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed backports.weakref-1.0rc1 markdown-2.2.0 tensorflow-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVfXb52HjK0f",
        "colab_type": "code",
        "outputId": "fc33d88c-4b5e-474b-a651-e8e80b718c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "!apt-get -qq install -y libfluidsynth1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package libfluidsynth1:amd64.\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 144433 files and directories currently installed.)\n",
            "Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9d7ab_cl05N",
        "colab_type": "code",
        "outputId": "3af372e4-6c62-4b98-97f6-e2f486cc7b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "!pip uninstall tensorflow\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.5.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/external/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.5.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9cq1DJCmJPK",
        "colab_type": "code",
        "outputId": "9aaf12e7-640d-4456-f54e-200cb703f587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "source": [
        "!pip install tensorflow==1.5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/79/a37d0b373757b4d283c674a64127bd8864d69f881c639b1ee5953e2d9301/tensorflow-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (44.4MB)\n",
            "\u001b[K     |████████████████████████████████| 44.4MB 72kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (1.16.2)\n",
            "Collecting tensorflow-tensorboard<1.6.0,>=1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/fa/91c06952517b4f1bc075545b062a4112e30cebe558a6b962816cb33efa27/tensorflow_tensorboard-1.5.1-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (0.34.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (3.10.0)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (1.0.1)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 56.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (3.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.5) (46.3.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=f9232c92921f974dd93576e7251b8ed218ffe057d703deffa5cc2b44c0f8b04c\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorflow-tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.5\n",
            "    Uninstalling bleach-3.1.5:\n",
            "      Successfully uninstalled bleach-3.1.5\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorflow-1.5.0 tensorflow-tensorboard-1.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIptqBzlmS23",
        "colab_type": "code",
        "outputId": "e9668217-0fde-441e-93ad-6b8a3488e94a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pGbUIT1nvhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#구글 드라이브로 부터 파일 가져오기\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCbJr_oYlHU_",
        "colab_type": "code",
        "outputId": "2b8ede58-a5cd-4895-dbfd-6ca6594c8e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "source": [
        "from PIL import Image # PIL 이미지 처리 기능 패키지\n",
        "import os, glob, numpy as np \n",
        "from sklearn.model_selection import train_test_split #데이터 분할해주는 함수 train, validation, test\n",
        "\n",
        "#경로 가져오기\n",
        "caltech_dir = \"/content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train\"\n",
        "#작은 카테고리\n",
        "categories = [\"Rusianblue\", \"boombey\", \"Persian\",\"Siberia\"]\n",
        "nb_classes = len(categories)  # len함수: 문자열 길이 반환\n",
        "\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "X = [] \n",
        "y = []\n",
        "\n",
        "for idx, cat in enumerate(categories):\n",
        "    \n",
        "    #one-hot 돌리기.\n",
        "    label = [0 for i in range(nb_classes)] \n",
        "    label[idx] = 1\n",
        "    \n",
        "    #cat = \"Rusianblue\", \"봄베이\", \"Persian\", \"Siberia\" 중 하나\n",
        "    image_dir = caltech_dir + \"/\" + cat\n",
        "    files = glob.glob(image_dir+\"/*.jpg\") #glob함수: 파일명을 문자열로 반환\n",
        "    print(cat, \" 파일 길이 : \", len(files))\n",
        "    for i, f in enumerate(files):\n",
        "        img = Image.open(f)\n",
        "        img = img.convert(\"RGB\")\n",
        "        img = img.resize((image_w, image_h))\n",
        "        #변환한 이미지 파일을 data로 \n",
        "        data = np.asarray(img) #numpy 배열로 변환\n",
        "\n",
        "        X.append(data)\n",
        "        y.append(label)\n",
        "\n",
        "        if i % 700 == 0:\n",
        "            print(cat, \" : \", f) #종류 : 데이터 개수\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "#1 0 0 0 이면 러시안블루\n",
        "#0 1 0 0 이면 봄베이 이런식\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "xy = (X_train, X_test, y_train, y_test)\n",
        "#경로에 train test data 를 multi_image_data 로 저장\n",
        "np.save(\"/content/drive/My Drive/Colab Notebooks/numpy_data/multi_image_data.npy\", xy) #xy 배열을 해당 경로에 저장\n",
        "\n",
        "print(\"ok\", len(y))\n",
        "print(\"ok\", len(X))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rusianblue  파일 길이 :  0\n",
            "boombey  파일 길이 :  0\n",
            "Persian  파일 길이 :  0\n",
            "Siberia  파일 길이 :  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-98e2891928df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#경로에 train test data 를 multi_image_data 로 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[0;32m-> 2122\u001b[0;31m                                               default_test_size=0.25)\n\u001b[0m\u001b[1;32m   2123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   1803\u001b[0m             \u001b[0;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[0;32m-> 1805\u001b[0;31m                                                 train_size)\n\u001b[0m\u001b[1;32m   1806\u001b[0m         )\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzGthTECgDbt",
        "colab_type": "code",
        "outputId": "e64f9f5b-fa58-4b63-d454-6c073fc3a5c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        }
      },
      "source": [
        "pip install keras==2.2.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/51/0192489a2614e8c6a22de860e43221e566f4bbd44a047ff48c2fdbc59373/Keras-2.2.1-py2.py3-none-any.whl (299kB)\n",
            "\r\u001b[K     |█                               | 10kB 22.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30kB 6.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 61kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (1.18.4)\n",
            "Collecting keras-preprocessing==1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/71/26/1e778ebd737032749824d5cba7dbd3b0cf9234b87ab5ec79f5f0403ca7e9/Keras_Preprocessing-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.1) (2.10.0)\n",
            "Collecting keras-applications==1.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/8f327deaa37a71caddb59b7b4aaa9d4b3e90c0e76f8c2d1572005278ddc5/Keras_Applications-1.0.4-py2.py3-none-any.whl (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 36.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 30kB 44.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 45.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 8.6MB/s \n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement keras-preprocessing>=1.1.0, but you'll have keras-preprocessing 1.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-preprocessing, keras-applications, keras\n",
            "  Found existing installation: Keras-Preprocessing 1.1.0\n",
            "    Uninstalling Keras-Preprocessing-1.1.0:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
            "  Found existing installation: Keras-Applications 1.0.8\n",
            "    Uninstalling Keras-Applications-1.0.8:\n",
            "      Successfully uninstalled Keras-Applications-1.0.8\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "Successfully installed keras-2.2.1 keras-applications-1.0.4 keras-preprocessing-1.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "keras_applications",
                  "keras_preprocessing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk56Zi3Fhbqh",
        "colab_type": "code",
        "outputId": "3ea0f128-3be0-4da3-df7a-48bcd463898b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "!pip install numpy==1.16.2\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 195kB/s \n",
            "\u001b[31mERROR: umap-learn 0.4.2 has requirement numpy>=1.17, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement keras-preprocessing>=1.1.0, but you'll have keras-preprocessing 1.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.18.4\n",
            "    Uninstalling numpy-1.18.4:\n",
            "      Successfully uninstalled numpy-1.18.4\n",
            "Successfully installed numpy-1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlOb9aCdll6g",
        "colab_type": "code",
        "outputId": "e8ddbc18-8111-4485-d56a-5c14296fb071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "import os, glob, numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend.tensorflow_backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "#config = tf.compat.v1.ConfigProto()\n",
        "config = tf.ConfigProto()\n",
        "#config = tf.config.experimental\n",
        "\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.Session(config=config)\n",
        "\n",
        "X_train, X_test, y_train, y_test = np.load('/content/drive/My Drive/Colab Notebooks/numpy_data/multi_image_data.npy')#저장된 파일에서 배열로 불러오기\n",
        "\n",
        "print(X_train.shape)\n",
        "#print(X_train.shape[0])\n",
        "print(y_train.shape)\n",
        "#print(y_train.shape[1])\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9ac5de9f90db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#config = tf.compat.v1.ConfigProto()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#config = tf.config.experimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'ConfigProto'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV9JalmhfRM5",
        "colab_type": "code",
        "outputId": "e03fdecb-935b-48aa-bf4e-f1164d52015e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "#\"러시안블루\", \"봄베이\", \"Persian\", \"Siberia\"\n",
        "categories = [\"러시안블루\", \"봄베이\", \"Persian\", \"Siberia\"]\n",
        "nb_classes = len(categories) \n",
        "\n",
        "#일반화\n",
        "X_train = X_train.astype(float) / 255\n",
        "X_test = X_test.astype(float) / 255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3b0d2751614f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#일반화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaDHaKTo30eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with K.tf_ops.device('/device:GPU:0'):\n",
        "  # CNN 이용하기\n",
        "\n",
        "  # 첫번째 convolution 레이어 추가 - 필터로 특징 뽑아줌\n",
        "\n",
        "    model = Sequential()\n",
        "    #Conv2D(필터수 / 커널 행,열/경계처리(출력 이미지= 입력 이미지 크기동일)/)\n",
        "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "  # 두번째 convolution 레이어 추가\n",
        "    model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "  \n",
        "  #Fully connected network 추가\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "    #평가기준 =accuracy\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model_dir = './model'\n",
        "    \n",
        "    if not os.path.exists(model_dir):\n",
        "        os.mkdir(model_dir)\n",
        "    \n",
        "    model_path = model_dir + '/multi_img_classification.model'\n",
        "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVz-EV1g32uJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gJrLBVA35wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#데이터셋이 적어서 validation을 그냥 test 데이터로 했습니다. \n",
        "#데이터셋이 충분하시면 이렇게 하시지 마시고 validation_split=0.2 이렇게 하셔서 테스트 셋으로 나누시길 권장합니다.\n",
        "\n",
        "#모델 학습시키기\n",
        "#입력데이터/라벨값/ 가중치 /학습횟수 /비슷한게 5번이상나오면 학습중단\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLD6i6eD37pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGPIj7py4FLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "x_len = np.arange(len(y_loss))\n",
        "\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "#학습에 대한 오차< 검증용 오차"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "___-ne9j_ccv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob, numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "caltech_dir = \"/content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others_test\"\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "\n",
        "X = []\n",
        "filenames = []\n",
        "name = []\n",
        "files = glob.glob(caltech_dir+\"/*.*\")\n",
        "model = load_model('./model/multi_img_classification.model')\n",
        "cnt=0\n",
        "for i, f in enumerate(files):\n",
        "    #print(f)\n",
        "    img = Image.open(f)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((image_w, image_h))\n",
        "    data = np.asarray(img)\n",
        "    filenames.append(f)\n",
        "    X.append(data)\n",
        "    prediction = model.predict(data[np.newaxis])\n",
        "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "    \n",
        "    for i in prediction:\n",
        "      pre_ans = i.argmax()  # 예측 레이블\n",
        "      pre_ans_str = ''\n",
        "      # 순서 \"Rusianblue\", \"봄베이\", \"Persian\", \"Siberia\"\n",
        "      if pre_ans == 0: pre_ans_str = \"Rusianblue\"\n",
        "      elif pre_ans == 1: pre_ans_str = \"봄베이\"\n",
        "      elif pre_ans == 2: pre_ans_str = \"Persian\"\n",
        "      else: pre_ans_str = \"Siberia\"\n",
        "      if i[0] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "      if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "      if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "      if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"/\")[8]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "      cnt += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_iZMN4d4Hw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob, numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "caltech_dir = \"/content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others_test\"\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "X = []\n",
        "filenames = []\n",
        "files = glob.glob(caltech_dir+\"/*.*\")\n",
        "\n",
        "for i, f in enumerate(files):\n",
        "    img = Image.open(f)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((image_w, image_h))\n",
        "    data = np.asarray(img)\n",
        "    filenames.append(f)\n",
        "    X.append(data)\n",
        "    \n",
        "    \n",
        "model = load_model('./model/multi_img_classification.model')\n",
        "\n",
        "prediction = model.predict(X)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "cnt = 0\n",
        "\n",
        "#이 비교는 그냥 파일들이 있으면 해당 파일과 비교. 카테고리와 함께 비교해서 진행하는 것은 _4 파일.\n",
        "for i in prediction:\n",
        "    pre_ans = i.argmax()  # 예측 레이블\n",
        "    print(i)\n",
        "    print(pre_ans)\n",
        "    pre_ans_str = ''\n",
        "    # 순서 \"Rusianblue\", \"봄베이\", \"Persian\", \"Siberia\"\n",
        "    if pre_ans == 0: pre_ans_str = \"Rusianblue\"\n",
        "    elif pre_ans == 1: pre_ans_str = \"봄베이\"\n",
        "    elif pre_ans == 2: pre_ans_str = \"Persian\"\n",
        "    else: pre_ans_str = \"Siberia\"\n",
        "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
        "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
        "    cnt += 1\n",
        "    # print(i.argmax()) #얘가 레이블 [1. 0. 0.] 이런식으로 되어 있는 것을 숫자로 바꿔주는 것.\n",
        "    # 즉 얘랑, 나중에 카테고리 데이터 불러와서 카테고리랑 비교를 해서 같으면 맞는거고, 아니면 틀린거로 취급하면 된다.\n",
        "    # 이걸 한 것은 _4.py에."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKnZ4RZ88AB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(3)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "# 원본 이미지 위치 \n",
        "optInputPath = '/content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/Bengal'\n",
        "# 늘릴 이미지가 저장될 위치\n",
        "optOutputPath = '/content/drive/My Drive/Colab Notebooks/Cats dataset/multi_img_data/imgs_others/train/PlusBengal'\n",
        "\n",
        "# 이미지 크기 조정 비율\n",
        "optRescale = 1./255\n",
        "# 이미지 회전 \n",
        "optRotationRange=10\n",
        "# 이미지 수평 이동\n",
        "optWidthShiftRange=0.2\n",
        "# 이미지 수직 이동\n",
        "optHeightShiftRange=0.2\n",
        "# 이미지 밀림 강도 \n",
        "optShearRange=0.5\n",
        "# 이미지 확대/ 축소 \n",
        "optZoomRange=[0.9,2.2]\n",
        "# 이미지 수평 뒤집기 \n",
        "optHorizontalFlip = True \n",
        "# 이미지 수직 뒤집기 \n",
        "optVerticalFlip = True\n",
        "optFillMode='nearest'\n",
        "# 이미지당 늘리는 갯수 \n",
        "optNbrOfIncreasePerPic = 5\n",
        "# 배치 수 \n",
        "optNbrOfBatchPerPic = 5\n",
        "\n",
        "'''\n",
        " 총 개수 optNbrOfIncreasePerPic * optNbrOfBatchPerPic \n",
        " 예 >\n",
        " 사진 1장에 \n",
        " optNbrOfIncreasePerPic = 5\n",
        " optNbrOfBatchPerPic = 5\n",
        " = 1 * 5 * 5 = 25장 생성  \n",
        "'''\n",
        "\n",
        "# 데이터셋 불러오기\n",
        "train_datagen = ImageDataGenerator(rescale=optRescale, \n",
        "                                   rotation_range=optRotationRange,\n",
        "                                   width_shift_range=optWidthShiftRange,\n",
        "                                   height_shift_range=optHeightShiftRange,\n",
        "                                   shear_range=optShearRange,\n",
        "                                   zoom_range=optZoomRange,\n",
        "                                   horizontal_flip=optHorizontalFlip,\n",
        "                                   vertical_flip=optVerticalFlip,\n",
        "                                   fill_mode=optFillMode)\n",
        "'''\n",
        "폴더가 존재하는지 확인하고\n",
        "없다면 생성 \n",
        "'''\n",
        "def checkFoler(path):\n",
        "    try:\n",
        "        if not(os.path.isdir(path)):\n",
        "            os.makedirs(os.path.join(path))\n",
        "    except OSError as e:\n",
        "        if e.errno != errno.EEXIST:                        \n",
        "            raise            \n",
        "\n",
        "def increaseImage(path ,folder):\n",
        "    for index in range(0,optNbrOfIncreasePerPic):                                   \n",
        "        img = load_img(path)\n",
        "        x = img_to_array(img)\n",
        "        x = x.reshape((1,) + x.shape)\n",
        "        i = 0\n",
        "        # 부풀리는 이미지를 저장할 폴다가 존재하는지 확인\n",
        "        # 없다면 생성 \n",
        "        checkFoler(optOutputPath+folder)               \n",
        "        print('index : ' + str(index))\n",
        "        for batch in train_datagen.flow(x, batch_size=1, save_to_dir=optOutputPath+folder, save_prefix='bengal', save_format='jpg'):\n",
        "            i += 1\n",
        "            print(folder + \" \" + str(i))\n",
        "            if i >= optNbrOfBatchPerPic: \n",
        "                break\n",
        "\n",
        "def generator(dirName):\n",
        "    checkFoler(optOutputPath)\n",
        "    try:\n",
        "        fileNames = os.listdir(dirName)\n",
        "        for fileName in fileNames:\n",
        "            fullFileName = os.path.join(dirName, fileName)\n",
        "            if os.path.isdir(fullFileName):                \n",
        "                generator(fullFileName)\n",
        "            else:\n",
        "                # 확장자 \n",
        "                ext = os.path.splitext(fullFileName)[-1]\n",
        "                # 폴더 이름 \n",
        "                folderName = os.path.splitext(fullFileName)[0].split('/')[-2]\n",
        "                if(ext == '.jpg'):                    \n",
        "                    increaseImage(fullFileName, folderName)               \n",
        "                    \n",
        "    except PermissionError:\n",
        "        pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "        generator(optInputPath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVEmlSNkF9BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import cv2\n",
        "\n",
        "CAM_ID = 0\n",
        "def capture(camid = CAM_ID):\n",
        "    cam = cv2.VideoCapture(camid)\n",
        "    if cam.isOpened() == False:\n",
        "        print ('cant open the cam (%d)' % camid)\n",
        "        return None\n",
        "\n",
        "    ret, frame = cam.read()\n",
        "    if frame is None:\n",
        "        print ('frame is not exist')\n",
        "        return None\n",
        "    \n",
        "    # png로 압축 없이 영상 저장 \n",
        "    cv2.imwrite('messigray.png',frame, params=[cv2.IMWRITE_PNG_COMPRESSION,0])\n",
        "    cam.release()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    capture()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3JAY80arbrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "cap = cv2.VideoCapture(0)\n",
        "cap.set(3,640) # set Width\n",
        "cap.set(4,480) # set Height\n",
        "while(True):\n",
        "    ret, frame = cap.read()\n",
        "    frame = cv2.flip(frame, -1) # Flip camera vertically\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    cv2.imshow('frame', frame)\n",
        "    cv2.imshow('gray', gray)\n",
        "    \n",
        "    k = cv2.waitKey(30) & 0xff\n",
        "    if k == 27: # press 'ESC' to quit\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGdToVCysI0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}